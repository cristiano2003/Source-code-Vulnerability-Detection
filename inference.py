from torch.utils.data import Dataset, DataLoader
import hashlib
import torch 
from transformers import AutoTokenizer

from src.classification.model.model import Model

# remove duplicates and conflicting labels
def clean_data(text_samples):
    hashmap = {}

    for i in range(len(text_samples)):
      sample_digest = hashlib.sha256(text_samples[i].encode('utf-8')).hexdigest()

      if hashmap.get(sample_digest) == None:
        hashmap[sample_digest] = i
      

   

    values = hashmap.values()

    cleaned_samples = [text_samples[val] for val in values]
    
    return cleaned_samples

code = '''static void goodG2B()
char * data ;
structType myStruct ;
char dataBuffer [ FILENAME_MAX ] = "" ;
data = dataBuffer;
strcat ( data , "c:\\temp\\file.txt" );
myStruct . structFirst = data;
goodG2BSink ( myStruct );
void goodG2BSink(structType myStruct)
char * data = myStruct . structFirst ;
HANDLE hFile ;
hFile = CreateFileA ( data , ( GENERIC_WRITE | GENERIC_READ ) , 0 , NULL , OPEN_ALWAYS , FILE_ATTRIBUTE_NORMAL , NULL );
if ( hFile != INVALID_HANDLE_VALUE )
CloseHandle ( hFile );'''

code = [code]
sample = clean_data(code)
tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
sample  = tokenizer(sample, padding=True, truncation=True, return_tensors='pt', max_length=512)

model = Model()
result = model(sample["input_ids"], sample["attention_mask"])
print(result)
