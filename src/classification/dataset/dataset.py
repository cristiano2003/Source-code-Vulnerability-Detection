from torch.utils.data import Dataset, DataLoader
import hashlib
import torch 
from transformers import AutoTokenizer

# remove duplicates and conflicting labels
def clean_data(text_samples, labels):
    hashmap = {}
    blacklist = []

    for i in range(len(text_samples)):
      sample_digest = hashlib.sha256(text_samples[i].encode('utf-8')).hexdigest()

      if hashmap.get(sample_digest) == None:
        hashmap[sample_digest] = (labels[i], i)
      else:
        if hashmap[sample_digest][0] != labels[i] and (sample_digest not in blacklist):
          blacklist.append(sample_digest)

    for blacklisted_sample in blacklist:
      hashmap.pop(blacklisted_sample)

    values = hashmap.values()

    cleaned_samples = [text_samples[val[1]] for val in values]
    cleaned_labels = [(labels[val[1]]) for val in values]

    return cleaned_samples, cleaned_labels


class VulDeePeckerDataset(Dataset):
    def __init__(self, file_name, tokenizer):
        self.tokenizer = tokenizer

        # open file and process raw data
        raw_data = ""
        with open(file_name) as f:
          raw_data = f.read()

        raw_data = raw_data.split("------------------------------\n")

        self.samples = []
        self.labels = []

        # seperate samples from labels and eliminate empty entries
        for sample in raw_data:
          sample = sample.strip().split("\n")[1:]

          if len(sample) < 3:
            continue

          self.samples.append("\n".join(sample[:-1]))
          self.labels.append(int(sample[-1]) - 1)
          
        # Clean Data
        self.samples, self.labels = clean_data(self.samples, self.labels)
    
        # Tokenize samples
        self.samples = self.tokenizer(self.samples, padding=True, truncation=True, return_tensors='pt', max_length=512)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        label = self.labels[idx]
        return self.samples["input_ids"][idx].squeeze(), self.samples["attention_mask"][idx].squeeze(), label