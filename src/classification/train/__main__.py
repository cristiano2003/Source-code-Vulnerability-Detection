from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.loggers import WandbLogger
import torch
import wandb
import pytorch_lightning as pl
from pathlib import Path
from ..dataset.dataset import *
from ..dataset.dataset import *
from ..model.model import *
import argparse
from ..model.model import *
from torch.utils.data import ConcatDataset, DataLoader


import os
os.environ["TOKENIZERS_PARALLELISM"] = "true"

def train():
    pl.seed_everything(42)
    
    dataset= VulDeePeckerDataset("data/data.txt")
    dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])
    train_dataset = dataset[0]
    val_dataset = dataset[1]

    train_loader = DataLoader(train_dataset, batch_size=16, num_workers=4, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, num_workers=4)

    model = Model()


    chkpt_path = "../checkpoints"
    checkpoint_callback = ModelCheckpoint(
        dirpath = str(chkpt_path),
        filename="CodeBert",
        save_top_k=1,
        verbose=True,
        monitor="val_loss",
    )
    wandb.login(key='53f5746150b2ce7b0552996cb6acc3beec6e487f')
    logger = WandbLogger(project="source-code-detection",
                                name="CodeBert",
                                log_model="all")

    lr_callback = LearningRateMonitor("step")

    trainer = pl.Trainer(
      logger=logger,
        callbacks=[checkpoint_callback, lr_callback],
        max_epochs=4,
        enable_progress_bar=True,
        enable_model_summary=True
    )

    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
    
if __name__ == "__main__":
    train()