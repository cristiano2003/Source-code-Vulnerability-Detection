from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.loggers import WandbLogger
import torch
import wandb
import pytorch_lightning as pl
from pathlib import Path
from rouge import Rouge
import datasets

from ..model.model import *
import argparse


def train():
    pl.seed_everything(42)




    model = NewsSummaryModel(lr=learning_rate)

    chkpt_path = "../checkpoints"
    checkpoint_callback = ModelCheckpoint(
        dirpath = str(chkpt_path),
        filename="Best-T5-{epoch}-{step}-{val_loss:.2f}",
        save_top_k=1,
        verbose=True,
        monitor="val_loss",
    )
    wandb.login(key=args.wandb_key)
    logger = WandbLogger(project="text-summarization",
                                name="T5",
                                log_model="all")

    lr_callback = LearningRateMonitor("step")

    trainer = pl.Trainer(
        logger=logger,
        callbacks=[checkpoint_callback, lr_callback],
        max_epochs=N_EPOCHS,
        enable_progress_bar=True
    )

    trainer.fit(model, data_module)
    
if __name__ == "__main__":
    train()