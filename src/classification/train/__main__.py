from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.loggers import WandbLogger
import torch
import wandb
import pytorch_lightning as pl
from pathlib import Path
from ..dataset.get_dataset import *
from ..model.model import *
import argparse
from ..model.model import *

def train():
    pl.seed_everything(42)
    
    dataset = VulDeePeckerDataset("data/cwe119_cgd.txt")
    dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])
    train_dataset = dataset[0]
    val_dataset = dataset[1]

    train_loader = DataLoader(train_dataset, batch_size=16, num_workers=4, persistent_workers=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=4, persistent_workers=True)

    model = Model()

    chkpt_path = "../checkpoints"
    checkpoint_callback = ModelCheckpoint(
        dirpath = str(chkpt_path),
        filename="CodeBert",
        save_top_k=1,
        verbose=True,
        monitor="val_loss",
    )
    # wandb.login(key='53f5746150b2ce7b0552996cb6acc3beec6e487f')
    # logger = WandbLogger(project="source-code-detection",
    #                             name="CodeBert",
    #                             log_model="all")

    lr_callback = LearningRateMonitor("step")

    trainer = pl.Trainer(
        #logger=logger,
        callbacks=[checkpoint_callback, lr_callback],
        max_epochs=10,
        enable_progress_bar=True,
        accelerator='gpu', devices=2, strategy="ddp_spawn"
    )

    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
    
if __name__ == "__main__":
    train()