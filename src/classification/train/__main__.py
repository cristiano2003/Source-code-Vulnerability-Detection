from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.loggers import WandbLogger
import torch
import wandb
import pytorch_lightning as pl
from pathlib import Path
from ..dataset.dataset import *
from ..dataset.dataset import *
from ..model.model import *
import argparse
from ..model.model import *
from torch.utils.data import ConcatDataset, DataLoader
from transformers import RobertaTokenizer

import os
os.environ["TOKENIZERS_PARALLELISM"] = "true"

def train(model_name, dataset_name):
    pl.seed_everything(42)
    if model_name == "codebert":
        tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
    elif model_name == "roberta":
        tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-base")
    elif model_name == "distilbert":
        tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')
    else:
        tokenizer = AutoTokenizer.from_pretrained("nvidia/megatron-bert-cased-345m")
        
    dataset= VulDeePeckerDataset("data/DATA CODE/java.txt", tokenizer)
    dataset = torch.utils.data.random_split(dataset, [0.85, 0.15])
    train_dataset = dataset[0]
    val_dataset = dataset[1]

    train_loader = DataLoader(train_dataset, batch_size=16, num_workers=4, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, num_workers=4)

    model = Model(model_name)


    chkpt_path = "../checkpoints"
    checkpoint_callback = ModelCheckpoint(
        dirpath = str(chkpt_path),
        filename="CodeBert-multi",
        save_top_k=1,
        verbose=True,
        monitor="val_loss",
    )
    wandb.login(key='53f5746150b2ce7b0552996cb6acc3beec6e487f')
    logger = WandbLogger(project="source-code-detection",
                                name=f"{dataset_name}-{model_name}",
                                log_model="all") 

    lr_callback = LearningRateMonitor("step")

    trainer = pl.Trainer(
       logger=logger,
        callbacks=[checkpoint_callback, lr_callback],
        max_epochs=25,
        enable_progress_bar=True,
        enable_model_summary=True
    )

    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
    
if __name__ == "__main__":
    train("roberta", "java")