from .codebert import *
import pytorch_lightning as pl
from transformers import AutoTokenizer, AutoModel
import torch.nn as nn
from torchmetrics.classification import MulticlassF1Score
from sklearn.metrics import f1_score

class Model(pl.LightningModule):
    def __init__(self):
        super().__init__()
        codebert_model = AutoModel.from_pretrained("microsoft/codebert-base")
        self.model = CodeBERTForVulnDetection(codebert = codebert_model)
        self.loss = nn.CrossEntropyLoss()
        self.f1 = MulticlassF1Score(num_classes=2)

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids, attention_mask)

    def training_step(self, batch, batch_idx):
        input_ids = batch[0]
        attention_mask = batch[1]
        labels = batch[2]
              
        outputs = self(input_ids, attention_mask)
        loss = self.loss(outputs, labels)
        with torch.no_grad():
            acc = (outputs.argmax(dim=1) == labels).float().mean()
            f1 = float(f1_score(labels.cpu(), outputs.argmax(dim=1).cpu(), average="macro"))
       
        self.log_dict({'train_loss':loss,
                       'train_acc': acc,
                       'F1_train_score': f1})
        
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids = batch[0]
        attention_mask = batch[1]
        labels = batch[2]
              
        outputs = self(input_ids, attention_mask)
        loss = self.loss(outputs, labels)
        with torch.no_grad():
            acc = (outputs.argmax(dim=1) == labels).float().mean()
            f1 = float(f1_score(labels.cpu(), outputs.argmax(dim=1).cpu(), average="macro"))
       
        self.log_dict({'val_loss':loss,
                       'val_acc': acc,
                       'F1_val_score': f1})
        
        return loss

    def configure_optimizers(self):
    
        return torch.optim.AdamW(self.model.parameters(), lr=1e-5, weight_decay=0.05)