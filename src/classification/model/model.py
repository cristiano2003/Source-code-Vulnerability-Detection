from .codebert import *
import pytorch_lightning as pl
from transformers import AutoTokenizer, AutoModel
import torch.nn as nn
from torchmetrics.classification import MulticlassF1Score


class Model(pl.LightningModule):
    def __init__(self):
        super().__init__()
        codebert_model = AutoModel.from_pretrained("microsoft/codebert-base")
        self.model = CodeBERTForVulnDetection(codebert = codebert_model)
        self.loss = nn.CrossEntropyLoss()
        self.f1 = MulticlassF1Score(num_classes=2)

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids, attention_mask)

    def training_step(self, batch, batch_idx):
        input_ids = batch[0]
        attention_mask = batch[1]
        labels = batch[2]
        
        outputs = self(input_ids, attention_mask)
        loss = self.loss(outputs, labels)
        # f1_score = self.f1(outputs, labels)
        # self.log_dict({'train_loss':loss,
        #                'F1 train score': f1_score})
        self.log("train loss", loss)
        
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids = batch[0]
        attention_mask = batch[1]
        labels = batch[2]
              
        outputs = self(input_ids, attention_mask)
        loss = self.loss(outputs, labels)
        # f1_score = self.f1(outputs, labels)
        # self.log_dict({'val_loss':loss,
        #                'F1 val score': f1_score})
        self.log("val loss", loss)
        
        return loss

    def configure_optimizers(self):
    
        return torch.optim.AdamW(self.model.parameters(), lr=1e-5, weight_decay=0.05)